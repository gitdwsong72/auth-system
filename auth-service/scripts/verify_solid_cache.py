#!/usr/bin/env python3
"""Solid Cache ì„¤ì • ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Solid Cacheê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤:
1. í…Œì´ë¸” ìƒì„± í™•ì¸
2. ì¸ë±ìŠ¤ ìƒì„± í™•ì¸
3. Cleanup í•¨ìˆ˜ í™•ì¸
4. ê¸°ë³¸ CRUD ì‘ì—… í…ŒìŠ¤íŠ¸
5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì„ íƒì‚¬í•­)

Usage:
    python scripts/verify_solid_cache.py
    python scripts/verify_solid_cache.py --benchmark  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í¬í•¨
"""

import asyncio
import sys
import time
from pathlib import Path

import asyncpg

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.shared.database.solid_cache import SolidCache


async def verify_table_exists(conn: asyncpg.Connection) -> bool:
    """solid_cache_entries í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤."""
    query = """
        SELECT EXISTS (
            SELECT FROM information_schema.tables
            WHERE table_schema = 'public'
            AND table_name = 'solid_cache_entries'
        )
    """
    exists = await conn.fetchval(query)
    return exists


async def verify_indexes(conn: asyncpg.Connection) -> dict:
    """ì¸ë±ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œë‹¤."""
    query = """
        SELECT indexname
        FROM pg_indexes
        WHERE tablename = 'solid_cache_entries'
        AND schemaname = 'public'
    """
    rows = await conn.fetch(query)
    indexes = [row["indexname"] for row in rows]

    return {
        "idx_solid_cache_expires": "idx_solid_cache_expires" in indexes,
        "idx_solid_cache_key_pattern": "idx_solid_cache_key_pattern" in indexes,
        "primary_key": "solid_cache_entries_pkey" in indexes,
    }


async def verify_cleanup_function(conn: asyncpg.Connection) -> bool:
    """cleanup_expired_cache() í•¨ìˆ˜ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤."""
    query = """
        SELECT EXISTS (
            SELECT FROM pg_proc
            WHERE proname = 'cleanup_expired_cache'
        )
    """
    exists = await conn.fetchval(query)
    return exists


async def test_basic_operations(pool: asyncpg.Pool) -> dict:
    """ê¸°ë³¸ CRUD ì‘ì—…ì„ í…ŒìŠ¤íŠ¸í•œë‹¤."""
    cache = SolidCache(pool)
    results = {}

    try:
        # 1. SET
        await cache.set("test_key", "test_value", ttl_seconds=60)
        results["set"] = "âœ… PASS"
    except Exception as e:
        results["set"] = f"âŒ FAIL: {e}"

    try:
        # 2. GET
        value = await cache.get("test_key")
        if value == "test_value":
            results["get"] = "âœ… PASS"
        else:
            results["get"] = f"âŒ FAIL: Expected 'test_value', got '{value}'"
    except Exception as e:
        results["get"] = f"âŒ FAIL: {e}"

    try:
        # 3. SET JSON
        data = {"user_id": 123, "name": "Test User"}
        await cache.set_json("test_json", data, ttl_seconds=60)
        results["set_json"] = "âœ… PASS"
    except Exception as e:
        results["set_json"] = f"âŒ FAIL: {e}"

    try:
        # 4. GET JSON
        cached_data = await cache.get_json("test_json")
        if cached_data == data:
            results["get_json"] = "âœ… PASS"
        else:
            results["get_json"] = "âŒ FAIL: Data mismatch"
    except Exception as e:
        results["get_json"] = f"âŒ FAIL: {e}"

    try:
        # 5. EXISTS
        exists = await cache.exists("test_key")
        if exists:
            results["exists"] = "âœ… PASS"
        else:
            results["exists"] = "âŒ FAIL: Key should exist"
    except Exception as e:
        results["exists"] = f"âŒ FAIL: {e}"

    try:
        # 6. TTL
        ttl = await cache.ttl("test_key")
        if 0 < ttl <= 60:
            results["ttl"] = f"âœ… PASS (TTL: {ttl}s)"
        else:
            results["ttl"] = f"âŒ FAIL: Invalid TTL {ttl}"
    except Exception as e:
        results["ttl"] = f"âŒ FAIL: {e}"

    try:
        # 7. DELETE
        await cache.delete("test_key")
        exists = await cache.exists("test_key")
        if not exists:
            results["delete"] = "âœ… PASS"
        else:
            results["delete"] = "âŒ FAIL: Key should not exist after delete"
    except Exception as e:
        results["delete"] = f"âŒ FAIL: {e}"

    try:
        # 8. DELETE PATTERN
        await cache.set("pattern:1", "value1", ttl_seconds=60)
        await cache.set("pattern:2", "value2", ttl_seconds=60)
        await cache.set("other:1", "value3", ttl_seconds=60)

        deleted = await cache.delete_pattern("pattern:%")
        if deleted >= 2:
            results["delete_pattern"] = f"âœ… PASS (Deleted {deleted} keys)"
        else:
            results["delete_pattern"] = f"âŒ FAIL: Expected 2+, deleted {deleted}"
    except Exception as e:
        results["delete_pattern"] = f"âŒ FAIL: {e}"

    try:
        # 9. GET STATS
        stats = await cache.get_stats()
        if "total_entries" in stats and "expired_entries" in stats:
            results["get_stats"] = f"âœ… PASS (Entries: {stats['total_entries']})"
        else:
            results["get_stats"] = "âŒ FAIL: Invalid stats format"
    except Exception as e:
        results["get_stats"] = f"âŒ FAIL: {e}"

    try:
        # 10. CLEANUP
        deleted = await cache.cleanup_expired()
        results["cleanup_expired"] = f"âœ… PASS (Cleaned up {deleted} entries)"
    except Exception as e:
        results["cleanup_expired"] = f"âŒ FAIL: {e}"

    # Cleanup test data
    try:
        await cache.delete("test_json")
        await cache.delete("other:1")
    except Exception:
        pass

    return results


async def benchmark_operations(pool: asyncpg.Pool, iterations: int = 100) -> dict:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•œë‹¤."""
    cache = SolidCache(pool)
    results = {}

    # 1. SET ì„±ëŠ¥
    start = time.perf_counter()
    for i in range(iterations):
        await cache.set(f"bench_key_{i}", f"value_{i}", ttl_seconds=300)
    elapsed = time.perf_counter() - start
    results["set_avg_ms"] = (elapsed / iterations) * 1000

    # 2. GET ì„±ëŠ¥
    start = time.perf_counter()
    for i in range(iterations):
        await cache.get(f"bench_key_{i}")
    elapsed = time.perf_counter() - start
    results["get_avg_ms"] = (elapsed / iterations) * 1000

    # 3. JSON SET ì„±ëŠ¥
    test_data = {"user_id": 123, "name": "Test", "roles": ["admin", "user"]}
    start = time.perf_counter()
    for i in range(iterations):
        await cache.set_json(f"bench_json_{i}", test_data, ttl_seconds=300)
    elapsed = time.perf_counter() - start
    results["set_json_avg_ms"] = (elapsed / iterations) * 1000

    # 4. JSON GET ì„±ëŠ¥
    start = time.perf_counter()
    for i in range(iterations):
        await cache.get_json(f"bench_json_{i}")
    elapsed = time.perf_counter() - start
    results["get_json_avg_ms"] = (elapsed / iterations) * 1000

    # Cleanup benchmark data
    await cache.delete_pattern("bench_key_%")
    await cache.delete_pattern("bench_json_%")

    return results


async def main(run_benchmark: bool = False):
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜."""
    # Database URL (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°)
    import os

    db_url = os.getenv(
        "DB_PRIMARY_DB_URL",
        "postgresql://auth_user:auth_pass@localhost:5433/auth_db?sslmode=disable",
    )

    print("=" * 80)
    print("Solid Cache ê²€ì¦ ì‹œì‘")
    print("=" * 80)
    print(f"Database: {db_url.split('@')[1] if '@' in db_url else 'unknown'}\n")

    try:
        # Connection Pool ìƒì„±
        pool = await asyncpg.create_pool(
            db_url,
            min_size=2,
            max_size=5,
        )

        async with pool.acquire() as conn:
            # 1. í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            print("ğŸ“‹ Step 1: í…Œì´ë¸” ì¡´ì¬ í™•ì¸")
            table_exists = await verify_table_exists(conn)
            if table_exists:
                print("   âœ… solid_cache_entries í…Œì´ë¸”ì´ ì¡´ì¬í•©ë‹ˆë‹¤\n")
            else:
                print("   âŒ solid_cache_entries í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                print(
                    "   ğŸ’¡ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì„¸ìš”: scripts/migrations/005_add_solid_cache.sql\n"
                )
                return

            # 2. ì¸ë±ìŠ¤ í™•ì¸
            print("ğŸ“‹ Step 2: ì¸ë±ìŠ¤ í™•ì¸")
            indexes = await verify_indexes(conn)
            for idx_name, exists in indexes.items():
                status = "âœ…" if exists else "âŒ"
                print(f"   {status} {idx_name}")
            print()

            # 3. Cleanup í•¨ìˆ˜ í™•ì¸
            print("ğŸ“‹ Step 3: Cleanup í•¨ìˆ˜ í™•ì¸")
            cleanup_exists = await verify_cleanup_function(conn)
            if cleanup_exists:
                print("   âœ… cleanup_expired_cache() í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤\n")
            else:
                print("   âŒ cleanup_expired_cache() í•¨ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n")

        # 4. ê¸°ë³¸ CRUD í…ŒìŠ¤íŠ¸
        print("ğŸ“‹ Step 4: ê¸°ë³¸ CRUD ì‘ì—… í…ŒìŠ¤íŠ¸")
        crud_results = await test_basic_operations(pool)
        for operation, result in crud_results.items():
            print(f"   {operation.upper()}: {result}")
        print()

        # 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì˜µì…˜)
        if run_benchmark:
            print("ğŸ“‹ Step 5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (100 iterations)")
            benchmark_results = await benchmark_operations(pool, iterations=100)
            for operation, avg_ms in benchmark_results.items():
                print(f"   {operation}: {avg_ms:.2f}ms")
            print()

        # ìµœì¢… í†µê³„
        cache = SolidCache(pool)
        stats = await cache.get_stats()
        print("ğŸ“Š Solid Cache í†µê³„:")
        print(f"   ì´ ì—”íŠ¸ë¦¬: {stats['total_entries']}")
        print(f"   ë§Œë£Œëœ ì—”íŠ¸ë¦¬: {stats['expired_entries']}")
        print(f"   ìŠ¤í† ë¦¬ì§€ í¬ê¸°: {stats['total_size_bytes'] / 1024:.2f} KB")
        print()

        # ê²°ë¡ 
        all_passed = all("âœ…" in str(r) for r in crud_results.values())
        if all_passed and table_exists and all(indexes.values()) and cleanup_exists:
            print("=" * 80)
            print("ğŸ‰ ëª¨ë“  ê²€ì¦ í†µê³¼! Solid Cacheê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("=" * 80)
        else:
            print("=" * 80)
            print("âš ï¸  ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨. ìœ„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            print("=" * 80)

        await pool.close()

    except asyncpg.InvalidCatalogNameError:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_url}")
        print("ğŸ’¡ DB_PRIMARY_DB_URL í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Solid Cache ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)",
    )
    args = parser.parse_args()

    asyncio.run(main(run_benchmark=args.benchmark))
