# Backpressure & Queue-based Traffic Control ì„¤ê³„

**ëª©ì **: ì‹œìŠ¤í…œ ìˆ˜ìš© í•œê³„ ì´ˆê³¼ ì‹œ ìš”ì²­ì„ ëŒ€ê¸°ì‹œì¼œ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ë³´

---

## ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ í•œê³„ ë¶„ì„

### 1. Connection Pool ê¸°ë°˜ ì„ê³„ì¹˜

#### Phase 1 ìˆ˜ì • ì „ (í˜„ì¬)
```python
# src/shared/database/connection.py
max_pool_size = 50
avg_query_time = 15ms (Permission ì¿¼ë¦¬ í¬í•¨)
bcrypt_blocking_time = 200ms

# ì„ê³„ì¹˜ ê³„ì‚°
ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥ ìš”ì²­ = 50 connections
ì•ˆì „ ì„ê³„ì¹˜ = 50 Ã— 0.8 = 40 connections  # 80% í™œìš©ë¥ 
```

**í˜„ì¬ í•œê³„**: ~200 RPS (bcrypt ë³‘ëª©)

#### Phase 1 ìˆ˜ì • í›„
```python
max_pool_size = 100
avg_query_time = 5ms (ìºì‹œ í™œìš©)
bcrypt_async_time = 20ms

# ì„ê³„ì¹˜ ê³„ì‚°
ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥ ìš”ì²­ = 100 connections
ì•ˆì „ ì„ê³„ì¹˜ = 100 Ã— 0.8 = 80 connections
```

**ê°œì„  í›„ í•œê³„**: ~1,500 RPS

---

### 2. CPU/Memory ê¸°ë°˜ ì„ê³„ì¹˜

#### CPU í•œê³„
```python
# ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ (4 vCPU)
workers = 4
max_concurrent_per_worker = 100  # asyncio tasks

# ì´ ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥
max_concurrent = 4 Ã— 100 = 400 requests

# ì•ˆì „ ì„ê³„ì¹˜ (70% CPU ì‚¬ìš©ë¥  ìœ ì§€)
safe_threshold = 400 Ã— 0.7 = 280 requests
```

#### Memory í•œê³„
```python
# ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ (4GB RAM)
app_base_memory = 500MB
per_request_memory = 1MB  # JWT decode, permission check ë“±

# ìµœëŒ€ ìš”ì²­ ìˆ˜
max_requests = (4000MB - 500MB) / 1MB = 3,500 requests

# ì•ˆì „ ì„ê³„ì¹˜ (80% ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ )
safe_threshold = 3,500 Ã— 0.8 = 2,800 requests
```

**ê²°ë¡ **: DB Connection Poolì´ ê°€ì¥ ì œí•œì ì¸ ìš”ì†Œ (100ê°œ)

---

### 3. Response Time ê¸°ë°˜ ì„ê³„ì¹˜ (ê¶Œì¥)

#### SLA ê¸°ë°˜ ì„ê³„ì¹˜ ì„¤ì •
```python
# SLA ëª©í‘œ
target_p95_latency = 100ms  # 95%ì˜ ìš”ì²­ì´ 100ms ì´ë‚´

# Little's Law: L = Î» Ã— W
# L (ë™ì‹œ ìš”ì²­) = Î» (RPS) Ã— W (í‰ê·  ì²˜ë¦¬ ì‹œê°„)

# Phase 1 ìˆ˜ì • í›„
avg_processing_time = 50ms = 0.05s
target_rps = 1,500

# í•„ìš”í•œ ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥
L = 1,500 Ã— 0.05 = 75 concurrent requests

# ì•ˆì „ ì„ê³„ì¹˜ (ë²„í¼ 20%)
safe_threshold = 75 Ã— 1.2 = 90 concurrent requests
```

---

## ğŸ¯ ê¶Œì¥ ì„ê³„ì¹˜ ì„¤ì • (í™˜ê²½ë³„)

### Development
```python
QUEUE_CONFIG = {
    "max_concurrent": 20,        # ë™ì‹œ ì²˜ë¦¬ í•œê³„
    "queue_capacity": 100,       # ëŒ€ê¸°ì—´ ìµœëŒ€ í¬ê¸°
    "wait_timeout": 10,          # ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    "reject_threshold": 120,     # ì´ˆê³¼ ì‹œ ì¦‰ì‹œ ê±°ë¶€
}
```

### Staging
```python
QUEUE_CONFIG = {
    "max_concurrent": 80,
    "queue_capacity": 500,
    "wait_timeout": 5,
    "reject_threshold": 580,
}
```

### Production (per instance)
```python
QUEUE_CONFIG = {
    "max_concurrent": 100,       # DB Poolê³¼ ë™ê¸°í™”
    "queue_capacity": 1000,      # 10ì´ˆì¹˜ ë²„í¼ (100 RPS ê°€ì •)
    "wait_timeout": 3,           # ë¹ ë¥¸ ì‹¤íŒ¨
    "reject_threshold": 1100,
    "priority_lanes": {
        "critical": 0.3,         # 30%ëŠ” ì¤‘ìš” ìš”ì²­ìš©
        "normal": 0.6,           # 60%ëŠ” ì¼ë°˜ ìš”ì²­ìš©
        "bulk": 0.1,             # 10%ëŠ” ë°°ì¹˜ ì‘ì—…ìš©
    }
}
```

---

## ğŸ—ï¸ êµ¬í˜„ ì•„í‚¤í…ì²˜

### Option 1: Application-Level Semaphore (ê¶Œì¥ - ê°„ë‹¨í•¨)

**ì¥ì **:
- êµ¬í˜„ ê°„ë‹¨, ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ
- ë©”ëª¨ë¦¬ ê¸°ë°˜, ë¹ ë¥¸ ì‘ë‹µ

**ë‹¨ì **:
- ì¸ìŠ¤í„´ìŠ¤ë³„ ë…ë¦½ (ì „ì—­ ì œì–´ ë¶ˆê°€)
- ì¬ì‹œì‘ ì‹œ ëŒ€ê¸°ì—´ ì†ì‹¤

#### êµ¬í˜„

```python
# src/shared/middleware/backpressure.py
import asyncio
import time
from typing import Optional
from fastapi import Request, Response, status
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware

class BackpressureMiddleware(BaseHTTPMiddleware):
    """
    ì‹œìŠ¤í…œ ìˆ˜ìš© í•œê³„ë¥¼ ë„˜ëŠ” ìš”ì²­ì„ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•˜ê³  ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    """

    def __init__(
        self,
        app,
        max_concurrent: int = 100,
        queue_capacity: int = 1000,
        wait_timeout: int = 3,
        reject_threshold: int = 1100,
    ):
        super().__init__(app)
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.queue_capacity = queue_capacity
        self.wait_timeout = wait_timeout
        self.reject_threshold = reject_threshold

        # ë©”íŠ¸ë¦­
        self._current_requests = 0
        self._queued_requests = 0
        self._rejected_requests = 0
        self._total_wait_time = 0.0

    async def dispatch(self, request: Request, call_next):
        # Health checkëŠ” bypass
        if request.url.path in ["/health", "/metrics"]:
            return await call_next(request)

        # ì¦‰ì‹œ ê±°ë¶€ (ì‹œìŠ¤í…œ ì™„ì „ ê³¼ë¶€í•˜)
        total_load = self._current_requests + self._queued_requests
        if total_load >= self.reject_threshold:
            self._rejected_requests += 1
            return JSONResponse(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                content={
                    "success": False,
                    "error": {
                        "code": "SYSTEM_OVERLOAD",
                        "message": "System is overloaded. Please try again later.",
                        "retry_after": 5,  # 5ì´ˆ í›„ ì¬ì‹œë„ ê¶Œì¥
                    }
                },
                headers={"Retry-After": "5"},
            )

        # ëŒ€ê¸°ì—´ ì´ˆê³¼ (503 ë°˜í™˜í•˜ë˜ ë” ë¹ ë¥¸ ì¬ì‹œë„ ê¶Œì¥)
        if self._queued_requests >= self.queue_capacity:
            return JSONResponse(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                content={
                    "success": False,
                    "error": {
                        "code": "QUEUE_FULL",
                        "message": "Service is busy. Please retry.",
                        "retry_after": 1,
                    }
                },
                headers={"Retry-After": "1"},
            )

        # ëŒ€ê¸°ì—´ì— ì¶”ê°€
        self._queued_requests += 1
        wait_start = time.time()

        try:
            # Semaphore íšë“ ëŒ€ê¸° (timeout ì ìš©)
            async with asyncio.timeout(self.wait_timeout):
                async with self.semaphore:
                    self._queued_requests -= 1
                    self._current_requests += 1

                    wait_time = time.time() - wait_start
                    self._total_wait_time += wait_time

                    # ëŒ€ê¸° ì‹œê°„ì´ ê¸¸ì—ˆë‹¤ë©´ í—¤ë”ì— í¬í•¨
                    response = await call_next(request)
                    if wait_time > 0.1:  # 100ms ì´ìƒ ëŒ€ê¸°
                        response.headers["X-Queue-Wait-Time"] = f"{wait_time:.3f}"

                    self._current_requests -= 1
                    return response

        except asyncio.TimeoutError:
            # ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ - 503 ë°˜í™˜
            self._queued_requests -= 1
            self._rejected_requests += 1
            return JSONResponse(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                content={
                    "success": False,
                    "error": {
                        "code": "QUEUE_TIMEOUT",
                        "message": f"Request timed out after {self.wait_timeout}s in queue.",
                        "retry_after": 2,
                    }
                },
                headers={"Retry-After": "2"},
            )

    def get_metrics(self) -> dict:
        """ëª¨ë‹ˆí„°ë§ìš© ë©”íŠ¸ë¦­"""
        return {
            "current_requests": self._current_requests,
            "queued_requests": self._queued_requests,
            "rejected_requests": self._rejected_requests,
            "avg_wait_time": (
                self._total_wait_time / max(1, self._current_requests)
            ),
            "utilization": self._current_requests / self.semaphore._value,
        }
```

#### ì ìš©

```python
# src/main.py
from src.shared.middleware.backpressure import BackpressureMiddleware
from src.shared.config import get_settings

settings = get_settings()

app = FastAPI(title="Auth Service")

# Backpressure Middleware ì¶”ê°€ (ê°€ì¥ ë¨¼ì €)
if settings.enable_backpressure:
    app.add_middleware(
        BackpressureMiddleware,
        max_concurrent=settings.backpressure_max_concurrent,
        queue_capacity=settings.backpressure_queue_capacity,
        wait_timeout=settings.backpressure_wait_timeout,
        reject_threshold=settings.backpressure_reject_threshold,
    )

# ë‹¤ë¥¸ ë¯¸ë“¤ì›¨ì–´ë“¤...
app.add_middleware(RateLimiterMiddleware, ...)
```

#### í™˜ê²½ ë³€ìˆ˜

```bash
# .env.production
ENABLE_BACKPRESSURE=true
BACKPRESSURE_MAX_CONCURRENT=100
BACKPRESSURE_QUEUE_CAPACITY=1000
BACKPRESSURE_WAIT_TIMEOUT=3
BACKPRESSURE_REJECT_THRESHOLD=1100
```

---

### Option 2: Redis-based Queue (ê¶Œì¥ - ë¶„ì‚° í™˜ê²½)

**ì¥ì **:
- ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°„ ì „ì—­ ì œì–´
- ì¬ì‹œì‘ í›„ì—ë„ ëŒ€ê¸°ì—´ ìœ ì§€
- ìš°ì„ ìˆœìœ„ í êµ¬í˜„ ê°€ëŠ¥

**ë‹¨ì **:
- Redis ì˜ì¡´ì„± ì¦ê°€
- ì•½ê°„ì˜ ì˜¤ë²„í—¤ë“œ

#### êµ¬í˜„

```python
# src/shared/middleware/redis_backpressure.py
import asyncio
import time
import uuid
from typing import Optional
from fastapi import Request, Response, status
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
from redis.asyncio import Redis

class RedisBackpressureMiddleware(BaseHTTPMiddleware):
    """
    Redis ê¸°ë°˜ ë¶„ì‚° ëŒ€ê¸°ì—´ ì‹œìŠ¤í…œ
    ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ê°€ ì „ì—­ ì„ê³„ì¹˜ë¥¼ ê³µìœ 
    """

    def __init__(
        self,
        app,
        redis: Redis,
        max_concurrent: int = 300,  # ì „ì²´ ì‹œìŠ¤í…œ ê¸°ì¤€
        queue_capacity: int = 3000,
        wait_timeout: int = 3,
        instance_id: Optional[str] = None,
    ):
        super().__init__(app)
        self.redis = redis
        self.max_concurrent = max_concurrent
        self.queue_capacity = queue_capacity
        self.wait_timeout = wait_timeout
        self.instance_id = instance_id or str(uuid.uuid4())[:8]

        # Redis í‚¤
        self.active_key = "backpressure:active"
        self.queue_key = "backpressure:queue"
        self.metrics_key = f"backpressure:metrics:{self.instance_id}"

    async def dispatch(self, request: Request, call_next):
        # Health check bypass
        if request.url.path in ["/health", "/metrics"]:
            return await call_next(request)

        request_id = str(uuid.uuid4())
        wait_start = time.time()

        try:
            # 1. í˜„ì¬ í™œì„± ìš”ì²­ ìˆ˜ í™•ì¸
            active_count = await self.redis.get(self.active_key)
            active_count = int(active_count) if active_count else 0

            # 2. ëŒ€ê¸°ì—´ í¬ê¸° í™•ì¸
            queue_size = await self.redis.llen(self.queue_key)

            # 3. ì¦‰ì‹œ ê±°ë¶€ íŒë‹¨
            if active_count + queue_size >= self.max_concurrent + self.queue_capacity:
                await self.redis.hincrby(self.metrics_key, "rejected", 1)
                return JSONResponse(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    content={
                        "success": False,
                        "error": {
                            "code": "SYSTEM_OVERLOAD",
                            "message": "System capacity exceeded.",
                            "retry_after": 5,
                            "active_requests": active_count,
                            "queue_size": queue_size,
                        }
                    },
                    headers={"Retry-After": "5"},
                )

            # 4. ëŒ€ê¸°ì—´ì— ì¶”ê°€
            if active_count >= self.max_concurrent:
                # ëŒ€ê¸° í•„ìš”
                await self.redis.rpush(self.queue_key, request_id)
                await self.redis.hincrby(self.metrics_key, "queued", 1)

                # í´ë§ìœ¼ë¡œ ìˆœì„œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì ìš©)
                timeout_at = time.time() + self.wait_timeout
                while time.time() < timeout_at:
                    # ëŒ€ê¸°ì—´ ì„ ë‘ í™•ì¸
                    first_in_queue = await self.redis.lindex(self.queue_key, 0)
                    if first_in_queue == request_id.encode():
                        # í™œì„± ìŠ¬ë¡¯ í™•ë³´ ì‹œë„
                        active = await self.redis.incr(self.active_key)
                        if active <= self.max_concurrent:
                            # ì„±ê³µ - ëŒ€ê¸°ì—´ì—ì„œ ì œê±°
                            await self.redis.lpop(self.queue_key)
                            break
                        else:
                            # ì‹¤íŒ¨ - ë‹¤ì‹œ ê°ì†Œ
                            await self.redis.decr(self.active_key)

                    await asyncio.sleep(0.05)  # 50ms ëŒ€ê¸°
                else:
                    # íƒ€ì„ì•„ì›ƒ
                    await self.redis.lrem(self.queue_key, 1, request_id)
                    await self.redis.hincrby(self.metrics_key, "timeout", 1)
                    return JSONResponse(
                        status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                        content={
                            "success": False,
                            "error": {
                                "code": "QUEUE_TIMEOUT",
                                "message": "Request timed out in queue.",
                                "retry_after": 2,
                            }
                        },
                        headers={"Retry-After": "2"},
                    )
            else:
                # ì¦‰ì‹œ ì²˜ë¦¬ ê°€ëŠ¥
                await self.redis.incr(self.active_key)

            # 5. ìš”ì²­ ì²˜ë¦¬
            wait_time = time.time() - wait_start
            await self.redis.hincrbyfloat(
                self.metrics_key, "total_wait_time", wait_time
            )

            try:
                response = await call_next(request)
                if wait_time > 0.1:
                    response.headers["X-Queue-Wait-Time"] = f"{wait_time:.3f}"
                return response
            finally:
                # í™œì„± ìš”ì²­ ê°ì†Œ
                await self.redis.decr(self.active_key)

        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì •ë¦¬
            await self.redis.lrem(self.queue_key, 1, request_id)
            await self.redis.decr(self.active_key)
            raise

    async def get_metrics(self) -> dict:
        """ì „ì—­ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        active = await self.redis.get(self.active_key)
        queue_size = await self.redis.llen(self.queue_key)

        metrics = await self.redis.hgetall(self.metrics_key)
        return {
            "active_requests": int(active) if active else 0,
            "queue_size": queue_size,
            "rejected": int(metrics.get(b"rejected", 0)),
            "queued": int(metrics.get(b"queued", 0)),
            "timeout": int(metrics.get(b"timeout", 0)),
            "total_wait_time": float(metrics.get(b"total_wait_time", 0)),
        }
```

---

### Option 3: Priority Queue (VIP ìš°ì„  ì²˜ë¦¬)

```python
# src/shared/middleware/priority_backpressure.py
from enum import IntEnum

class RequestPriority(IntEnum):
    CRITICAL = 0   # Health check, ê´€ë¦¬ì ìš”ì²­
    HIGH = 1       # ë¡œê·¸ì¸, í† í° ê°±ì‹ 
    NORMAL = 2     # ì¼ë°˜ API
    LOW = 3        # ë°°ì¹˜ ì‘ì—…

class PriorityBackpressureMiddleware(BaseHTTPMiddleware):
    """
    ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëŒ€ê¸°ì—´
    """

    def __init__(self, app, redis: Redis, **config):
        super().__init__(app)
        self.redis = redis
        self.config = config

        # ìš°ì„ ìˆœìœ„ë³„ í
        self.queue_keys = {
            RequestPriority.CRITICAL: "backpressure:queue:critical",
            RequestPriority.HIGH: "backpressure:queue:high",
            RequestPriority.NORMAL: "backpressure:queue:normal",
            RequestPriority.LOW: "backpressure:queue:low",
        }

    def get_priority(self, request: Request) -> RequestPriority:
        """ìš”ì²­ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        path = request.url.path

        # Critical (í•­ìƒ ì²˜ë¦¬)
        if path in ["/health", "/metrics"]:
            return RequestPriority.CRITICAL

        # High (ì¸ì¦ ê´€ë ¨)
        if path in ["/api/v1/auth/login", "/api/v1/auth/refresh"]:
            return RequestPriority.HIGH

        # Normal (ì¼ë°˜ API)
        if request.method in ["GET", "POST", "PUT", "DELETE"]:
            # ê´€ë¦¬ìëŠ” Highë¡œ ìŠ¹ê²©
            if request.headers.get("X-User-Role") == "admin":
                return RequestPriority.HIGH
            return RequestPriority.NORMAL

        # Low (ë°°ì¹˜, ëŒ€ëŸ‰ ì‘ì—…)
        return RequestPriority.LOW

    async def dispatch(self, request: Request, call_next):
        priority = self.get_priority(request)

        # Criticalì€ bypass
        if priority == RequestPriority.CRITICAL:
            return await call_next(request)

        # ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€ ë° ì²˜ë¦¬
        # (êµ¬í˜„ ë¡œì§ì€ RedisBackpressureMiddlewareì™€ ìœ ì‚¬í•˜ë˜,
        #  ìš°ì„ ìˆœìœ„ë³„ë¡œ ë³„ë„ í ì‚¬ìš©)
        ...
```

#### ìš°ì„ ìˆœìœ„ë³„ í• ë‹¹

```python
PRIORITY_ALLOCATION = {
    "critical": 0.1,   # 10% ìŠ¬ë¡¯ ì˜ˆì•½
    "high": 0.4,       # 40% ìŠ¬ë¡¯
    "normal": 0.4,     # 40% ìŠ¬ë¡¯
    "low": 0.1,        # 10% ìŠ¬ë¡¯
}

# 100 ìŠ¬ë¡¯ ê¸°ì¤€
# Critical: 10ê°œ (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
# High: 40ê°œ
# Normal: 40ê°œ
# Low: 10ê°œ (ì—¬ìœ  ìˆì„ ë•Œë§Œ)
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ & ë©”íŠ¸ë¦­

### Metrics Endpoint

```python
# src/api/routes/metrics.py
from fastapi import APIRouter, Depends
from src.shared.middleware.backpressure import BackpressureMiddleware

router = APIRouter(prefix="/metrics", tags=["monitoring"])

@router.get("/backpressure")
async def get_backpressure_metrics(
    middleware: BackpressureMiddleware = Depends(get_backpressure_middleware)
):
    """
    Backpressure ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¡°íšŒ
    """
    metrics = middleware.get_metrics()
    return {
        "success": True,
        "data": {
            "current_requests": metrics["current_requests"],
            "queued_requests": metrics["queued_requests"],
            "rejected_requests": metrics["rejected_requests"],
            "avg_wait_time_ms": metrics["avg_wait_time"] * 1000,
            "utilization_percent": metrics["utilization"] * 100,
            "status": (
                "healthy" if metrics["utilization"] < 0.7
                else "warning" if metrics["utilization"] < 0.9
                else "critical"
            )
        }
    }
```

### Prometheus í†µí•©

```python
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
backpressure_active_requests = Gauge(
    "backpressure_active_requests",
    "Current number of active requests"
)

backpressure_queued_requests = Gauge(
    "backpressure_queued_requests",
    "Current number of queued requests"
)

backpressure_rejected_total = Counter(
    "backpressure_rejected_total",
    "Total number of rejected requests"
)

backpressure_wait_time = Histogram(
    "backpressure_wait_time_seconds",
    "Time spent waiting in queue",
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 3.0, 5.0]
)

# Middlewareì—ì„œ ì—…ë°ì´íŠ¸
class BackpressureMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        backpressure_active_requests.set(self._current_requests)
        backpressure_queued_requests.set(self._queued_requests)

        # ... ì²˜ë¦¬ ...

        backpressure_wait_time.observe(wait_time)
```

### Grafana Dashboard

```json
{
  "dashboard": {
    "title": "Backpressure Monitoring",
    "panels": [
      {
        "title": "Active vs Queued Requests",
        "targets": [
          {
            "expr": "backpressure_active_requests",
            "legendFormat": "Active"
          },
          {
            "expr": "backpressure_queued_requests",
            "legendFormat": "Queued"
          }
        ]
      },
      {
        "title": "Queue Wait Time (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, backpressure_wait_time_seconds)"
          }
        ]
      },
      {
        "title": "Rejection Rate",
        "targets": [
          {
            "expr": "rate(backpressure_rejected_total[5m])"
          }
        ]
      }
    ]
  }
}
```

---

## ğŸ”” ì•Œë¦¼ ê·œì¹™

```yaml
# alerts/backpressure.yml
groups:
  - name: backpressure
    interval: 30s
    rules:
      # Critical: ê±°ë¶€ìœ¨ ë†’ìŒ
      - alert: HighRejectionRate
        expr: rate(backpressure_rejected_total[5m]) > 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High request rejection rate"
          description: "{{ $value }} requests/sec are being rejected"

      # Warning: ëŒ€ê¸°ì—´ ì¦ê°€
      - alert: QueueBuildup
        expr: backpressure_queued_requests > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Queue is building up"
          description: "{{ $value }} requests waiting in queue"

      # Critical: ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼
      - alert: HighQueueWaitTime
        expr: histogram_quantile(0.95, backpressure_wait_time_seconds) > 1
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "Queue wait time exceeds 1s"
          description: "P95 wait time is {{ $value }}s"

      # Warning: ë†’ì€ í™œìš©ë¥ 
      - alert: HighUtilization
        expr: (backpressure_active_requests / 100) > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "System utilization above 85%"
          description: "Consider scaling out"
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### Test 1: ì •ìƒ ë¶€í•˜ (Utilization < 70%)

```python
# tests/load/test_backpressure_normal.py
import asyncio
from locust import HttpUser, task, between

class NormalLoadUser(HttpUser):
    wait_time = between(0.5, 1.5)

    @task
    def login(self):
        self.client.post("/api/v1/auth/login", json={
            "email": "test@example.com",
            "password": "pass"
        })
```

**ì‹¤í–‰**:
```bash
locust -f test_backpressure_normal.py \
  --users 70 \
  --spawn-rate 10 \
  --run-time 5m
```

**ì˜ˆìƒ ê²°ê³¼**:
- âœ… ëª¨ë“  ìš”ì²­ ì„±ê³µ (0% ê±°ë¶€)
- âœ… Queue Wait Time < 10ms
- âœ… Response Time P95 < 100ms

---

### Test 2: ì„ê³„ì¹˜ ë„ë‹¬ (Utilization 80-100%)

```bash
locust -f test_backpressure_normal.py \
  --users 120 \
  --spawn-rate 20 \
  --run-time 5m
```

**ì˜ˆìƒ ê²°ê³¼**:
- âœ… ëŒ€ë¶€ë¶„ ì„±ê³µ (< 5% ê±°ë¶€)
- âš ï¸ Queue Wait Time 100-500ms
- âš ï¸ Response Time P95 200-300ms
- âœ… ì‹œìŠ¤í…œ ì•ˆì • (í¬ë˜ì‹œ ì—†ìŒ)

---

### Test 3: ê³¼ë¶€í•˜ (Utilization > 150%)

```bash
locust -f test_backpressure_normal.py \
  --users 200 \
  --spawn-rate 50 \
  --run-time 5m
```

**ì˜ˆìƒ ê²°ê³¼**:
- âš ï¸ 30-50% ê±°ë¶€ (503 ì‘ë‹µ)
- âš ï¸ Queue Wait Time > 1s (timeout)
- âœ… ì‹œìŠ¤í…œ ë³´í˜¸ (í¬ë˜ì‹œ ì—†ìŒ)
- âœ… ì²˜ë¦¬ëœ ìš”ì²­ì€ ì •ìƒ ì‘ë‹µ

---

### Test 4: Spike (ê¸‰ì¦ íŠ¸ë˜í”½)

```python
# tests/load/test_spike.py
from locust import HttpUser, task, between, events

class SpikeUser(HttpUser):
    wait_time = between(0.1, 0.5)

    @task
    def stress(self):
        self.client.get("/api/v1/users/me")

@events.test_start.add_listener
def on_test_start(environment, **kwargs):
    # 0 â†’ 500 users in 10 seconds
    pass
```

**ì˜ˆìƒ ê²°ê³¼**:
- âš ï¸ ì²˜ìŒ 10ì´ˆ: ê±°ë¶€ìœ¨ ì¦ê°€
- âœ… 10ì´ˆ ì´í›„: ì•ˆì •í™” (íê°€ ì†Œí™”)
- âœ… ì‹œìŠ¤í…œ ë³µêµ¬ (ìë™ ì¡°ì ˆ)

---

## ğŸ’¡ Best Practices

### 1. ì ì ˆí•œ ì„ê³„ì¹˜ ì„¤ì •

```python
# âŒ ë„ˆë¬´ ë‚®ìŒ - ë¦¬ì†ŒìŠ¤ ë‚­ë¹„
max_concurrent = 20  # DB Pool 100ì¸ë°?

# âŒ ë„ˆë¬´ ë†’ìŒ - ë³´í˜¸ íš¨ê³¼ ì—†ìŒ
max_concurrent = 500  # DB Pool 100ì¸ë°?

# âœ… ì ì ˆ - ê°€ì¥ ì œí•œì ì¸ ë¦¬ì†ŒìŠ¤ ê¸°ì¤€
max_concurrent = DB_POOL_SIZE Ã— 0.8 = 80
```

### 2. Timeout ì„¤ì •

```python
# âŒ ë„ˆë¬´ ê¸¸ë©´ - ì‚¬ìš©ì ê²½í—˜ ë‚˜ì¨
wait_timeout = 30

# âŒ ë„ˆë¬´ ì§§ìœ¼ë©´ - ë¶ˆí•„ìš”í•œ ê±°ë¶€
wait_timeout = 0.5

# âœ… ì ì ˆ - ì‚¬ìš©ì ì¸ë‚´ì‹¬ ê³ ë ¤
wait_timeout = 3  # 3ì´ˆë©´ ì¶©ë¶„
```

### 3. Graceful Degradation

```python
# ë¶€í•˜ì— ë”°ë¼ ê¸°ëŠ¥ ì œí•œ
async def dispatch(self, request: Request, call_next):
    utilization = self._current_requests / self.max_concurrent

    if utilization > 0.9:
        # 90% ì´ìƒ: ì½ê¸° ì „ìš© ëª¨ë“œ
        if request.method not in ["GET", "HEAD"]:
            return JSONResponse(
                status_code=503,
                content={"error": "Read-only mode due to high load"}
            )

    if utilization > 0.95:
        # 95% ì´ìƒ: ìºì‹œëœ ì‘ë‹µë§Œ
        cached = await self.get_cached_response(request)
        if cached:
            return cached

    # ì •ìƒ ì²˜ë¦¬
    return await call_next(request)
```

### 4. Circuit Breaker í†µí•©

```python
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
async def process_request(request):
    # ì—°ì† 5ë²ˆ ì‹¤íŒ¨ ì‹œ 60ì´ˆê°„ ì°¨ë‹¨
    return await call_next(request)
```

---

## ğŸ“‹ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: ê¸°ë³¸ Semaphore (ì¦‰ì‹œ)

```bash
# 1ì¼ ì‘ì—…
âœ… Application-level Semaphore
âœ… 503 ì‘ë‹µ + Retry-After
âœ… ê¸°ë³¸ ë©”íŠ¸ë¦­
```

### Phase 2: Redis Queue (1ì£¼ì¼ ë‚´)

```bash
# 3ì¼ ì‘ì—…
âœ… Redis ê¸°ë°˜ ë¶„ì‚° ëŒ€ê¸°ì—´
âœ… ì „ì—­ ì„ê³„ì¹˜ ì œì–´
âœ… Prometheus ë©”íŠ¸ë¦­
```

### Phase 3: Priority Queue (1ê°œì›” ë‚´)

```bash
# 5ì¼ ì‘ì—…
âœ… ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬
âœ… VIP ë ˆì¸
âœ… Graceful Degradation
```

---

## ğŸ¯ ê¸°ëŒ€ íš¨ê³¼

### Before (Backpressure ì—†ìŒ)

```
# ê³¼ë¶€í•˜ ì‹œ
â”œâ”€ RPS: ë¶ˆì•ˆì • (50 ~ 500 fluctuation)
â”œâ”€ Response Time: 10ì´ˆ+ (timeout)
â”œâ”€ Error Rate: 80%+ (connection refused)
â””â”€ System: í¬ë˜ì‹œ ìœ„í—˜
```

### After (Backpressure ì ìš©)

```
# ê³¼ë¶€í•˜ ì‹œ
â”œâ”€ RPS: ì•ˆì •ì  (max 100, ë‚˜ë¨¸ì§€ëŠ” 503)
â”œâ”€ Response Time: 100ms (ì²˜ë¦¬ëœ ìš”ì²­)
â”œâ”€ Rejection Rate: 50% (503 with Retry-After)
â””â”€ System: ì•ˆì • (ë³´í˜¸ë¨)
```

**ê²°ë¡ **: ì¼ë¶€ ìš”ì²­ì€ ê±°ë¶€ë˜ì§€ë§Œ, **ì‹œìŠ¤í…œì€ í•­ìƒ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€**

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ìœ ì‚¬ ì‚¬ë¡€

1. **AWS API Gateway Throttling**
   - Burst: 5,000 RPS
   - Steady: 10,000 RPS
   - ì´ˆê³¼ ì‹œ: 429 Too Many Requests

2. **Google Cloud Load Balancer**
   - Connection Limits
   - Queue Depth: 1,000
   - Timeout: 30s

3. **Nginx Connection Limiting**
   ```nginx
   limit_conn_zone $binary_remote_addr zone=addr:10m;
   limit_conn addr 10;  # IPë‹¹ 10 ì—°ê²°
   ```

### Little's Law í™œìš©

```
L = Î» Ã— W

L: í‰ê·  ë™ì‹œ ìš”ì²­ ìˆ˜
Î»: í‰ê·  RPS
W: í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)

ì˜ˆì‹œ:
1000 RPS Ã— 0.05s = 50 concurrent requests
```

---

## âœ… Checklist

### êµ¬í˜„ ì „

- [ ] í˜„ì¬ ì‹œìŠ¤í…œ í•œê³„ ì¸¡ì • (Load Test)
- [ ] DB Connection Pool í¬ê¸° í™•ì¸
- [ ] Redis ìš©ëŸ‰ ê³„íš
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì¤€ë¹„

### êµ¬í˜„ ì¤‘

- [ ] Semaphore ë˜ëŠ” Redis Queue ì„ íƒ
- [ ] ì„ê³„ì¹˜ ì„¤ì • (max_concurrent)
- [ ] Timeout ì„¤ì • (wait_timeout)
- [ ] 503 ì‘ë‹µ í¬ë§· ì •ì˜
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ êµ¬í˜„

### êµ¬í˜„ í›„

- [ ] Load Testë¡œ ê²€ì¦
- [ ] ì•Œë¦¼ ê·œì¹™ ì„¤ì •
- [ ] ë¬¸ì„œí™” (Retry ì •ì±…)
- [ ] ìš´ì˜ ê°€ì´ë“œ ì‘ì„±

---

**ì‘ì„± ë‚ ì§œ**: 2026-02-11
**ë¬¸ì„œ ë²„ì „**: 1.0
